{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7-as8mRB4CA"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "\n",
        "class ChatData(Dataset):\n",
        "    def __init__(self, path:str, tokenizer):\n",
        "        self.data = json.load(open(path, \"r\"))\n",
        "\n",
        "        self.X = []\n",
        "        for i in self.data:\n",
        "            for j in i['dialog']:\n",
        "                self.X.append(j['text'])\n",
        "\n",
        "        for idx, i in enumerate(self.X):\n",
        "            try:\n",
        "                self.X[idx] = \"<startofstring> \"+i+\" <bot>: \"+self.X[idx+1]+\" <endofstring>\"\n",
        "            except:\n",
        "                break\n",
        "\n",
        "        self.X = self.X[:5000]\n",
        "\n",
        "        print(self.X[0])\n",
        "\n",
        "        self.X_encoded = tokenizer(self.X,max_length=40, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        self.input_ids = self.X_encoded['input_ids']\n",
        "        self.attention_mask = self.X_encoded['attention_mask']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.input_ids[idx], self.attention_mask[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nhp08HUbc6S4",
        "outputId": "e41221d4-0ef1-4b2e-c8ae-e602d1086058"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1 dialogues from /content/chat__Data.json\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 10%|█         | 1/10 [00:30<04:34, 30.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " hello how are you  <bot>:   <bot>: \n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 20%|██        | 2/10 [00:42<02:38, 19.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " hello how are you  <bot>: , and, and, and, and,, and,\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 30%|███       | 3/10 [00:50<01:38, 14.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " hello how are you  <bot>:  \n",
            "Epoch 4/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 40%|████      | 4/10 [00:56<01:05, 10.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " hello how are you  <bot>:  \n",
            "Epoch 5/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 50%|█████     | 5/10 [01:02<00:47,  9.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " hello how are you  <bot>:  \n",
            "Epoch 6/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 60%|██████    | 6/10 [01:08<00:33,  8.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " hello how are you  <bot>:  \n",
            "Epoch 7/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 70%|███████   | 7/10 [01:16<00:23,  7.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " hello how are you  <bot>:  \n",
            "Epoch 8/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 80%|████████  | 8/10 [01:29<00:19,  9.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " hello how are you  <bot>:  \n",
            "Epoch 9/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 90%|█████████ | 9/10 [01:36<00:08,  8.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " hello how are you  <bot>:  \n",
            "Epoch 10/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "100%|██████████| 10/10 [01:43<00:00, 10.34s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " hello how are you  <bot>: ,\n",
            "Inference from the model:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot:  hi  <bot>: ,,,,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot:  how are you  <bot>: ,,,,,,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot:  name  <bot>: ,,,,,,,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot:  go  <bot>: ,,,,,,,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot:  I love iphone! i just bought new iphone!  <bot>:  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot:  hi  <bot>:  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot:  your name  <bot>: ,,\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import tqdm\n",
        "import torch\n",
        "\n",
        "class ChatData(Dataset):\n",
        "    def __init__(self, json_file, tokenizer, max_length=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        # Load data from JSON file\n",
        "        with open(json_file, 'r') as file:\n",
        "            self.data = json.load(file)\n",
        "\n",
        "        # Print some info to debug\n",
        "        print(f\"Loaded {len(self.data)} dialogues from {json_file}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        dialogue = self.data[idx][\"dialog\"]\n",
        "\n",
        "        # Extract input and target texts from the dialogue\n",
        "        input_texts = []\n",
        "        target_texts = []\n",
        "        for message in dialogue:\n",
        "            if message[\"sender\"] == \"participant1\":\n",
        "                input_texts.append(message[\"text\"])\n",
        "            elif message[\"sender\"] == \"participant2\":\n",
        "                target_texts.append(message[\"text\"])\n",
        "\n",
        "        # Combine input and target texts into conversations\n",
        "        conversations = []\n",
        "        for input_text, target_text in zip(input_texts, target_texts):\n",
        "            input_text = \"<startofstring> \" + input_text + \" <bot>: \"\n",
        "            target_text = target_text + \" <endofstring>\"\n",
        "            conversations.append((input_text, target_text))\n",
        "\n",
        "        # Tokenize inputs\n",
        "        tokenized_conversations = []\n",
        "        for input_text, target_text in conversations:\n",
        "            input_tokens = self.tokenizer.encode(input_text, max_length=self.max_length, truncation=True, padding='max_length')\n",
        "            target_tokens = self.tokenizer.encode(target_text, max_length=self.max_length, truncation=True, padding='max_length')\n",
        "            tokenized_conversations.append({\n",
        "                \"input_ids\": input_tokens,\n",
        "                \"attention_mask\": [1 if token != self.tokenizer.pad_token_id else 0 for token in input_tokens],\n",
        "                \"target_ids\": target_tokens\n",
        "            })\n",
        "\n",
        "        return tokenized_conversations\n",
        "\n",
        "def train(chatData, model, optim):\n",
        "    epochs = 10\n",
        "\n",
        "    for epoch in tqdm.tqdm(range(epochs)):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        model.train()\n",
        "        for batch in chatData:\n",
        "            for conv in batch:\n",
        "                X = torch.tensor(conv[\"input_ids\"]).unsqueeze(0).to(device)\n",
        "                a = torch.tensor(conv[\"attention_mask\"]).unsqueeze(0).to(device)\n",
        "                optim.zero_grad()\n",
        "                loss = model(X, attention_mask=a, labels=X)[0]\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "        torch.save(model.state_dict(), \"model_state.pt\")\n",
        "        print(infer(\"hello how are you\"))\n",
        "\n",
        "def infer(inp):\n",
        "    inp = \"<startofstring> \"+inp+\" <bot>: \"\n",
        "    inp = tokenizer(inp, return_tensors=\"pt\")\n",
        "    X = inp[\"input_ids\"].to(device)\n",
        "    a = inp[\"attention_mask\"].to(device)\n",
        "    output = model.generate(X, attention_mask=a)\n",
        "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return output\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\",\n",
        "                                \"bos_token\": \"<startofstring>\",\n",
        "                                \"eos_token\": \"<endofstring>\"})\n",
        "tokenizer.add_tokens([\"<bot>:\"])\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "chatData = ChatData(\"/content/chat__Data.json\", tokenizer)\n",
        "chatData = DataLoader(chatData, batch_size=1)\n",
        "\n",
        "optim = Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "print(\"Training....\")\n",
        "train(chatData, model, optim)\n",
        "\n",
        "print(\"Inference from the model:\")\n",
        "while True:\n",
        "    inp = input(\"User: \")\n",
        "    if inp.lower() == 'exit':\n",
        "        break\n",
        "    response = infer(inp)\n",
        "    print(\"Bot:\", response)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}